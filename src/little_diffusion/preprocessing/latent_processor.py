import os
import json
import torch
import logging
import numpy as np
from pathlib import Path
from typing import List, Optional, Union, Tuple
from PIL import Image
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import AutoencoderKL
import torch.nn.functional as F

logger = logging.getLogger(__name__)


class SquarePadResize:
    """
    æ™ºèƒ½ç¼©æ”¾å¡«å…… (æ”¯æŒ Image å’Œ Mask åŒæ­¥å¤„ç†)ï¼š
    1. ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œè®©é•¿è¾¹ = target_size
    2. çŸ­è¾¹ç”¨æŒ‡å®šé¢œè‰²å¡«å…… (Pad) åˆ° target_size
    """

    def __init__(
        self,
        target_size: int,
        img_fill: tuple = (255, 255, 255),
        mask_fill: float = 0.01,
    ):
        self.target_size = target_size
        self.img_fill = img_fill
        # 0.01 æ˜¯æˆ‘ä»¬ä¸ºçº¯è‰²ç™½è¾¹åˆ†é…çš„æä½æƒé‡
        self.mask_fill = int(mask_fill * 255)

    def __call__(
        self, img: Image.Image, mask: Image.Image
    ) -> Tuple[Image.Image, Image.Image]:
        w, h = img.size

        # 1. è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ (åŸºäºé•¿è¾¹)
        ratio = self.target_size / max(w, h)
        new_w, new_h = int(w * ratio), int(h * ratio)

        # 2. åŒæ­¥ç¼©æ”¾
        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        mask = mask.resize(
            (new_w, new_h), Image.Resampling.NEAREST
        )  # Mask å»ºè®®ç”¨ NEAREST é˜²æ­¢è¾¹ç¼˜æ¨¡ç³Š

        # 3. åˆ›å»ºæ­£æ–¹å½¢ç”»å¸ƒ
        new_img = Image.new("RGB", (self.target_size, self.target_size), self.img_fill)
        new_mask = Image.new("L", (self.target_size, self.target_size), self.mask_fill)

        # 4. å±…ä¸­ç²˜è´´
        paste_x = (self.target_size - new_w) // 2
        paste_y = (self.target_size - new_h) // 2

        new_img.paste(img, (paste_x, paste_y))
        new_mask.paste(mask, (paste_x, paste_y))

        return new_img, new_mask


class JSONImageDataset(Dataset):
    """
    å‡çº§ç‰ˆæ•°æ®é›†ï¼šè¯»å– Image å’Œå¯¹åº”çš„ Weight Mask
    """

    def __init__(
        self,
        metadata_path: Union[str, Path],
        image_root: Union[str, Path],
        image_size: int = 1024,
    ):
        self.image_root = Path(image_root)
        self.image_size = image_size

        with open(metadata_path, "r", encoding="utf-8") as f:
            self.metadata = json.load(f)

        logger.info(f"ğŸ“š Loaded dataset index with {len(self.metadata)} items.")

        self.smart_resize = SquarePadResize(
            image_size, img_fill=(255, 255, 255), mask_fill=0.01
        )
        self.to_tensor = transforms.ToTensor()
        self.normalize = transforms.Normalize([0.5], [0.5])

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        image_path = self.image_root / item["file_path"]

        # å‡è®¾ mask ä¸ image åŒåï¼Œä½†åç¼€æ˜¯ .pngï¼Œä¸”å­˜åœ¨äº masks æ–‡ä»¶å¤¹ä¸‹
        # è¿™é‡Œéœ€è¦ä¸ä½ ä¸‹ä¸€æ­¥çš„ vision_processor äº§å‡ºå¯¹é½
        mask_path = (
            self.image_root.parent
            / "masks"
            / Path(item["file_path"]).with_suffix(".png").name
        )
        class_id = item["class_id"]

        try:
            img = Image.open(image_path).convert("RGB")

            # å¦‚æœ Mask ä¸å­˜åœ¨ (å…¼å®¹è€æ•°æ®)ï¼Œå°±å»ºä¸€ä¸ªå…¨ä¸º 1.0 çš„ Dummy Mask
            if mask_path.exists():
                mask = Image.open(mask_path).convert("L")
            else:
                mask = Image.new("L", img.size, 255)

            # åŒæ­¥ Resize å’Œ Pad
            img_padded, mask_padded = self.smart_resize(img, mask)

            # è½¬ Tensor (ToTensor ä¼šè‡ªåŠ¨æŠŠ 0~255 è½¬ä¸º 0.0~1.0)
            img_tensor = self.normalize(self.to_tensor(img_padded))
            mask_tensor = self.to_tensor(mask_padded)  # å½¢çŠ¶: (1, 1024, 1024)

            return img_tensor, mask_tensor, class_id

        except Exception as e:
            logger.error(f"âŒ Corrupted image {image_path}: {e}")
            return (
                torch.zeros(3, self.image_size, self.image_size),
                torch.zeros(1, self.image_size, self.image_size),
                -1,
            )


class VAEProcessor:
    # __init__ ä¿æŒä¸å˜ï¼Œç•¥ ...
    def __init__(
        self,
        model_name: str = "madebyollin/sdxl-vae-fp16-fix",
        device: Optional[str] = None,
        scaling_factor: float = 0.13025,
        use_fp16: bool = True,
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.scaling_factor = scaling_factor
        if use_fp16 and self.device != "cpu":
            if torch.cuda.is_bf16_supported():
                self.dtype = torch.bfloat16
            else:
                self.dtype = torch.float16
        else:
            self.dtype = torch.float32

        logger.info(f"ğŸš€ Loading VAE: {model_name}")
        try:
            self.vae = AutoencoderKL.from_pretrained(
                model_name, torch_dtype=self.dtype
            ).to(self.device)
            self.vae.eval()
            self.vae.requires_grad_(False)
        except Exception as e:
            logger.error(f"âŒ VAE Load Failed: {e}")
            raise e

    @torch.no_grad()
    def process_dataset(
        self,
        metadata_path: str,
        image_root: str,
        output_path: str,
        image_size: int = 1024,
        batch_size: int = 4,
        num_workers: int = 4,
    ):
        if os.path.dirname(output_path):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

        dataset = JSONImageDataset(metadata_path, image_root, image_size)
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )

        all_latents = []
        all_masks = []  # ğŸ‘ˆ æ–°å¢ï¼šå­˜å‚¨å¤„ç†å¥½çš„ Mask
        all_labels = []

        logger.info("ğŸ“¸ Starting VAE Encoding & Mask Downsampling...")

        for batch_imgs, batch_masks, batch_labels in tqdm(dataloader, desc="Encoding"):
            pixel_values = batch_imgs.to(self.device, dtype=self.dtype)

            # 1. VAE ç¼–ç  RGB å›¾åƒ
            dist = self.vae.encode(pixel_values).latent_dist
            latents = dist.sample() * self.scaling_factor

            # 2. Mask ä¸‹é‡‡æ ·ï¼(1024x1024 -> 128x128)
            # ä½¿ç”¨ Average Poolingï¼Œè¿™æ ·è¾¹ç¼˜çš„æƒé‡ä¼šå¹³æ»‘è¿‡æ¸¡
            masks_downsampled = F.avg_pool2d(batch_masks, kernel_size=8, stride=8)

            # 3. æ¬å› CPU
            all_latents.append(latents.float().cpu())
            all_masks.append(masks_downsampled.float().cpu())  # ğŸ‘ˆ æ”¶é›† Mask
            all_labels.append(batch_labels.long().cpu())

        final_latents = torch.cat(all_latents, dim=0)
        final_masks = torch.cat(all_masks, dim=0)
        final_labels = torch.cat(all_labels, dim=0)

        # æ‰“åŒ…ä¿å­˜
        payload = {
            "latents": final_latents,
            "masks": final_masks,  # ğŸ‘ˆ æ–°å¢ï¼šMask ä¸€å¹¶æ‰“åŒ…ï¼
            "labels": final_labels,
            "scaling_factor": self.scaling_factor,
            "image_size": image_size,
            "latent_size": final_latents.shape[-1],
        }

        torch.save(payload, output_path)

        logger.info(f"âœ… Saved processed data to {output_path}")
        logger.info(f"ğŸ“Š Latents Shape: {final_latents.shape}")
        logger.info(f"ğŸ­ Masks Shape: {final_masks.shape}")  # åº”è¯¥æ˜¯ (N, 1, 128, 128)
        logger.info(f"ğŸ·ï¸ Labels Shape: {final_labels.shape}")

    @torch.no_grad()
    def decode(self, latents: torch.Tensor) -> List[Image.Image]:
        """
        è§£ç å·¥å…· (ç”¨äº Sample é˜¶æ®µ)
        """
        latents = latents.to(self.device, dtype=self.dtype)
        latents = latents / self.scaling_factor

        image = self.vae.decode(latents).sample
        image = (image / 2 + 0.5).clamp(0, 1)

        image = image.cpu().permute(0, 2, 3, 1).float().numpy()

        output_images = []
        for i in range(image.shape[0]):
            img_np = (image[i] * 255).round().astype(np.uint8)
            output_images.append(Image.fromarray(img_np))

        return output_images
