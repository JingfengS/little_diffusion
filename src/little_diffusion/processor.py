import os
import json
import torch
import logging
import numpy as np
from pathlib import Path
from typing import List, Optional, Union
from PIL import Image
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import AutoencoderKL

logger = logging.getLogger(__name__)

class SquarePadResize:
    """
    æ™ºèƒ½ç¼©æ”¾å¡«å……ï¼š
    1. ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œè®©é•¿è¾¹ = target_size
    2. çŸ­è¾¹ç”¨ç™½è‰²å¡«å…… (Pad) åˆ° target_size
    ç»“æœï¼šä¸€å¼ ä¸å¤±çœŸã€ä¸è¢«è£å‰ªçš„æ­£æ–¹å½¢å›¾ç‰‡
    """
    def __init__(self, target_size: int, fill_color: tuple = (255, 255, 255)):
        self.target_size = target_size
        self.fill_color = fill_color

    def __call__(self, img: Image.Image) -> Image.Image:
        w, h = img.size
        
        # 1. è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ (åŸºäºé•¿è¾¹)
        ratio = self.target_size / max(w, h)
        new_w = int(w * ratio)
        new_h = int(h * ratio)
        
        # 2. ç¼©æ”¾
        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        
        # 3. åˆ›å»ºæ­£æ–¹å½¢ç”»å¸ƒ
        new_img = Image.new("RGB", (self.target_size, self.target_size), self.fill_color)
        
        # 4. å±…ä¸­ç²˜è´´
        paste_x = (self.target_size - new_w) // 2
        paste_y = (self.target_size - new_h) // 2
        new_img.paste(img, (paste_x, paste_y))
        
        return new_img
class JSONImageDataset(Dataset):
    """
    å‡çº§ç‰ˆæ•°æ®é›†ï¼šè¯»å– dataset.json ç´¢å¼•ï¼Œæ”¯æŒ Label
    """
    def __init__(
        self,
        metadata_path: Union[str, Path],
        image_root: Union[str, Path],
        image_size: int = 1024,
    ):
        self.image_root = Path(image_root)
        self.image_size = image_size
        
        # åŠ è½½ç´¢å¼•æ–‡ä»¶
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
            
        logger.info(f"ğŸ“š Loaded dataset index with {len(self.metadata)} images.")

        # VAE é¢„å¤„ç†: Resize (LANCZOS) -> CenterCrop -> Normalize
        # æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬ä¹‹å‰çš„é¢„å¤„ç†å·²ç»åšè¿‡ Letterbox å’Œ Resizeï¼Œ
        # è¿™é‡Œçš„ CenterCrop ä¸»è¦æ˜¯ä¸ºäº†é˜²å¾¡æ€§ç¼–ç¨‹ï¼Œé˜²æ­¢æœ‰æ¼ç½‘ä¹‹é±¼
        self.transform = transforms.Compose([
            SquarePadResize(image_size, fill_color=(255, 255, 255)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        image_path = self.image_root / item['file_path']
        class_id = item['class_id']
        
        try:
            img = Image.open(image_path).convert("RGB")
            tensor = self.transform(img)
            # è¿”å›: (Pixel_Tensor, Label_ID)
            return tensor, class_id
        except Exception as e:
            logger.error(f"âŒ Corrupted image {image_path}: {e}")
            # è¿”å›å…¨0å ä½ï¼Œå¹¶åœ¨ Label è®¾ä¸º -1 (éœ€è¦åœ¨ Collate æ—¶è¿‡æ»¤ï¼Œæˆ–è€…ç®€å•ç‚¹ç›´æ¥å¿½ç•¥é”™è¯¯)
            return torch.zeros(3, self.image_size, self.image_size), -1

class VAEProcessor:
    """
    VAE å¤„ç†å¼•æ“ï¼šè´Ÿè´£å°†å›¾ç‰‡è½¬ä¸º Latents å¹¶æ‰“åŒ…ä¿å­˜
    """
    def __init__(self,
                 model_name: str = "madebyollin/sdxl-vae-fp16-fix", # ğŸ‘ˆ å‡çº§ä¸º SDXL VAE
                 device: Optional[str] = None,
                 scaling_factor: float = 0.13025, # ğŸ‘ˆ SDXL çš„ç¼©æ”¾å› å­æ˜¯ 0.13025
                 use_fp16: bool = True):
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.scaling_factor = scaling_factor
        
        # 5070 Ti ä¼˜å…ˆä½¿ç”¨ BF16ï¼Œå¦‚æœä¸æ”¯æŒåˆ™å›é€€åˆ° FP16
        if use_fp16 and self.device != 'cpu':
            if torch.cuda.is_bf16_supported():
                self.dtype = torch.bfloat16
                logger.info("ğŸš€ Using BFloat16 for VAE processing (Ampere/Hopper Optimized)")
            else:
                self.dtype = torch.float16
                logger.info("ğŸš€ Using Float16 for VAE processing")
        else:
            self.dtype = torch.float32

        logger.info(f"ğŸš€ Loading VAE: {model_name}")
        try:
            self.vae = AutoencoderKL.from_pretrained(model_name, torch_dtype=self.dtype).to(self.device)
            self.vae.eval()
            self.vae.requires_grad_(False)
        except Exception as e:
            logger.error(f"âŒ VAE Load Failed: {e}")
            raise e
    
    @torch.no_grad()
    def process_dataset(self, 
                        metadata_path: str, 
                        image_root: str, 
                        output_path: str, 
                        image_size: int = 512, 
                        batch_size: int = 4, 
                        num_workers: int = 4):
        """
        è¯»å– dataset.json -> VAE Encode -> ä¿å­˜ Latents + Labels
        """
        if os.path.dirname(output_path):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

        dataset = JSONImageDataset(metadata_path, image_root, image_size)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
        
        all_latents = []
        all_labels = []
        
        logger.info("ğŸ“¸ Starting VAE Encoding...")
        
        for batch_imgs, batch_labels in tqdm(dataloader, desc="Encoding"):
            # 1. æ¬è¿å›¾ç‰‡åˆ° GPU
            pixel_values = batch_imgs.to(self.device, dtype=self.dtype)
            
            # 2. VAE ç¼–ç 
            # SDXL VAE è¾“å‡ºåˆ†å¸ƒï¼Œé‡‡æ ·å¹¶ç¼©æ”¾
            dist = self.vae.encode(pixel_values).latent_dist
            latents = dist.sample() * self.scaling_factor
            
            # 3. æ¬å› CPU (çœæ˜¾å­˜)
            all_latents.append(latents.float().cpu()) # å­˜ä¸º FP32 ä¿è¯ç²¾åº¦ï¼Œè®­ç»ƒæ—¶å†è½¬ BF16
            all_labels.append(batch_labels.long().cpu())
            
        # 4. æ‹¼æ¥å¤§å¼ é‡
        final_latents = torch.cat(all_latents, dim=0)
        final_labels = torch.cat(all_labels, dim=0)
        
        # 5. æ‰“åŒ…ä¿å­˜
        payload = {
            "latents": final_latents,
            "labels": final_labels,
            "scaling_factor": self.scaling_factor,
            "image_size": image_size,
            "latent_size": final_latents.shape[-1]
        }
        
        torch.save(payload, output_path)
        
        logger.info(f"âœ… Saved processed data to {output_path}")
        logger.info(f"ğŸ“Š Latents Shape: {final_latents.shape}")
        logger.info(f"ğŸ·ï¸ Labels Shape: {final_labels.shape}")
        
    @torch.no_grad()
    def decode(self, latents: torch.Tensor) -> List[Image.Image]:
        """
        è§£ç å·¥å…· (ç”¨äº Sample é˜¶æ®µ)
        """
        latents = latents.to(self.device, dtype=self.dtype)
        latents = latents / self.scaling_factor
        
        image = self.vae.decode(latents).sample
        image = (image / 2 + 0.5).clamp(0, 1)
        
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        
        output_images = []
        for i in range(image.shape[0]):
            img_np = (image[i] * 255).round().astype(np.uint8)
            output_images.append(Image.fromarray(img_np))
        
        return output_images