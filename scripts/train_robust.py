import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import logging
import argparse
import sys
import time
import signal
from pathlib import Path

# å¼•å…¥æˆ‘ä»¬çš„å·¥ä¸šçº§æ¨¡å—
from little_diffusion.models.config import DiTConfig
from little_diffusion.models.dit import DiT

# ================= ğŸš€ 5070 Ti æé€Ÿæ¨¡å¼è®¾ç½® =================
# å¼€å¯ TF32 (Ampere/Hopper/Blackwell ä¸“å±)
torch.set_float32_matmul_precision('high')
# æŠ‘åˆ¶ç¼–è¯‘å™ªéŸ³
torch._dynamo.config.suppress_errors = True

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ================= ğŸ› ï¸ å‚æ•°è§£æ =================
def get_args():
    parser = argparse.ArgumentParser(description="ğŸš€ Robust DiT Trainer with Resume & Triton")
    
    # åŸºç¡€é…ç½®
    parser.add_argument("--name", type=str, default="dit_test_run", help="Experiment name")
    parser.add_argument("--data", type=str, required=True, help="Path to arknights_latents_1024.pt")
    parser.add_argument("--output_dir", type=str, default="checkpoints/arknights", help="Directory to save checkpoints")
    
    # è®­ç»ƒè¶…å‚
    parser.add_argument("--epochs", type=int, default=1000)
    parser.add_argument("--batch_size", type=int, default=16, help="Adjust based on VRAM (16-32 for 5070Ti)")
    parser.add_argument("--lr", type=float, default=5e-5)
    parser.add_argument("--save_every", type=int, default=100, help="Save checkpoint every X epochs")
    
    # ç»­è®­æ§åˆ¶
    parser.add_argument("--resume", type=str, default="latest", help="Path to checkpoint or 'latest' to auto-resume")
    parser.add_argument("--force_restart", action="store_true", help="Ignore existing checkpoints and start over")
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument("--debug", action="store_true", help="Run with small model for testing")
    
    return parser.parse_args()

# ================= ğŸ’¾ Checkpoint ç®¡ç†å™¨ =================
class CheckpointManager:
    def __init__(self, save_dir, experiment_name):
        self.save_dir = Path(save_dir) / experiment_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.name = experiment_name
        self.latest_path = self.save_dir / "arknights_latest_checkpoint.pth"

    def save(self, model, optimizer, epoch, loss, config, is_best=False):
        """ä¿å­˜å®Œæ•´çŠ¶æ€"""
        # å¦‚æœæ¨¡å‹è¢« compile è¿‡ï¼Œå®ƒçš„ state_dict key ä¼šå¸¦æœ‰ "_orig_mod." å‰ç¼€
        # æˆ‘ä»¬éœ€è¦å»é™¤å®ƒï¼Œä»¥ä¾¿æœªæ¥åŠ è½½æ—¶ä¸å— compile çŠ¶æ€å½±å“
        raw_model = model._orig_mod if hasattr(model, '_orig_mod') else model
        
        state = {
            'epoch': epoch,
            'model_state_dict': raw_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': config.model_dump(), # ä¿å­˜ Pydantic Config
            'rng_state': torch.get_rng_state(),
            'cuda_rng_state': torch.cuda.get_rng_state(),
            'timestamp': time.time()
        }
        
        # 1. ä¿å­˜ä¸º latest (è¦†ç›–)
        torch.save(state, self.latest_path)
        
        # 2. ä¿å­˜ä¸º epoch å†å² (å½’æ¡£)
        epoch_path = self.save_dir / f"epoch_{epoch:04d}.pth"
        torch.save(state, epoch_path)
        
        logger.info(f"ğŸ’¾ Saved Checkpoint: Epoch {epoch} | Loss: {loss:.4f}")

    def load(self, path, model, optimizer=None):
        """åŠ è½½å®Œæ•´çŠ¶æ€"""
        if path == 'latest':
            path = self.latest_path
        
        path = Path(path)
        if not path.exists():
            logger.warning(f"âš ï¸ Checkpoint not found: {path}")
            return 0 # Start from epoch 0

        logger.info(f"â™»ï¸ Loading checkpoint from {path}...")
        checkpoint = torch.load(path, map_location='cpu') # å…ˆåŠ è½½åˆ° CPU çœæ˜¾å­˜
        
        # åŠ è½½æ¨¡å‹æƒé‡
        msg = model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        logger.info(f"   -> Model Weights Loaded: {msg}")
        
        # åŠ è½½ä¼˜åŒ–å™¨
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info("   -> Optimizer State Restored")
            
        # æ¢å¤éšæœºç§å­ (ç¡®ä¿å¤ç°æ€§)
        if 'rng_state' in checkpoint:
            torch.set_rng_state(checkpoint['rng_state'])
            if torch.cuda.is_available():
                torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])

        start_epoch = checkpoint['epoch'] + 1
        logger.info(f"âœ… Successfully Resumed from Epoch {start_epoch}")
        return start_epoch

# ================= ğŸ§  ä¸»ç¨‹åº =================
def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # 1. å‡†å¤‡æ•°æ®
    logger.info(f"ğŸ“¦ Loading dataset from {args.data}...")
    data_payload = torch.load(args.data, map_location="cpu")
    
    if isinstance(data_payload, dict):
        all_latents = data_payload['latents'] # (N, 4, 128, 128)
        all_labels = data_payload['labels']   # (N,)
        # è‡ªåŠ¨è·å–ç±»åˆ«æ•°
        num_classes = int(torch.max(all_labels).item()) + 1
    else:
        raise ValueError("Unsupported .pt format")
        
    logger.info(f"ğŸ“Š Dataset: {len(all_latents)} images, {num_classes} classes")
    
    dataset = TensorDataset(all_latents, all_labels)
    loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=4, 
        pin_memory=True,
        persistent_workers=True
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹ Config
    if args.debug:
        logger.warning("ğŸ› DEBUG MODE: Using Tiny DiT")
        config = DiTConfig(
            input_size=128, patch_size=2, hidden_size=64, depth=2, num_heads=4, num_classes=num_classes + 1
        )
    else:
        # æ ‡å‡† Small é…ç½®
        config = DiTConfig(
            input_size=128, patch_size=2, hidden_size=384, depth=12, num_heads=6, num_classes=num_classes + 1
        )
        
    model = DiT(config).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    params = sum(p.numel() for p in model.parameters()) / 1e6
    logger.info(f"ğŸ§  Model Initialized ({params:.2f}M params)")

    # 3. ä¼˜åŒ–å™¨ (å¯ç”¨ Fused)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0, fused=True)
    criterion = nn.MSELoss()

    # 4. Checkpoint ç®¡ç†
    ckpt_manager = CheckpointManager(args.output_dir, args.name)
    start_epoch = 0
    
    # å°è¯•æ¢å¤è®­ç»ƒ
    if not args.force_restart:
        start_epoch = ckpt_manager.load(args.resume, model, optimizer)
        # å°†ä¼˜åŒ–å™¨çŠ¶æ€ç§»åŠ¨åˆ° GPU (å› ä¸º load æ˜¯åœ¨ cpu åšçš„)
        for state in optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.to(device)

    # 5. ç¼–è¯‘æ¨¡å‹ (Resume ä¹‹åå†ç¼–è¯‘)
    logger.info("ğŸ”¥ Compiling model with Triton (mode='max-autotune')...")
    # max-autotune å¯èƒ½ä¼šæ…¢ï¼Œå¦‚æœä½ è§‰å¾—å¡ä½å¤ªä¹…ï¼Œå¯ä»¥æ”¹æˆ 'default'
    model = torch.compile(model, mode="max-autotune") 

    # 6. ä¿¡å·æ•è· (Ctrl+C)
    def signal_handler(sig, frame):
        logger.info("\nğŸ›‘ Interrupt received! Saving emergency checkpoint...")
        ckpt_manager.save(model, optimizer, epoch, avg_loss, config)
        sys.exit(0)
    signal.signal(signal.SIGINT, signal_handler)

    # 7. è®­ç»ƒå¾ªç¯
    logger.info(f"ğŸ¬ Training Start: Epoch {start_epoch} -> {args.epochs}")
    model.train()
    
    for epoch in range(start_epoch, args.epochs):
        epoch_loss = 0.0
        steps = 0
        progress_bar = tqdm(loader, desc=f"Epoch {epoch+1}/{args.epochs}", leave=True)
        
        for latents, labels in progress_bar:
            latents = latents.to(device, non_blocking=True).to(torch.bfloat16) # BF16
            labels = labels.to(device, non_blocking=True)
            
            # --- Diffusion Forward (Simple Linear Schedule) ---
            t = torch.randint(0, 1000, (latents.shape[0],), device=device)
            noise = torch.randn_like(latents)
            
            # ç®€å•çš„åŠ å™ª (ä»¥åå¯ä»¥æ¢æˆæ›´å¤æ‚çš„ Scheduler)
            # x_t = (1-alpha) * x + alpha * noise
            alpha = (t.view(-1, 1, 1, 1) / 1000.0) 
            x_t = (1 - alpha) * latents + alpha * noise
            
            target = noise # é¢„æµ‹å™ªå£° (Epsilon-Prediction)
            
            optimizer.zero_grad(set_to_none=True)
            
            # --- Mixed Precision Training (BF16) ---
            with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                pred = model(x_t, t, labels)
                loss = criterion(pred, target)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            steps += 1
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        avg_loss = epoch_loss / steps
        
        # --- å®šæœŸä¿å­˜ ---
        if (epoch + 1) % args.save_every == 0:
            ckpt_manager.save(model, optimizer, epoch, avg_loss, config)
            
    # è®­ç»ƒç»“æŸä¿å­˜
    ckpt_manager.save(model, optimizer, args.epochs-1, avg_loss, config)
    logger.info("ğŸ Training Finished Successfully!")

if __name__ == "__main__":
    main()