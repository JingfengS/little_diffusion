import argparse
import torch
import os
import time
import logging
import signal
import sys
from pathlib import Path
from torch.utils.data import DataLoader, TensorDataset
from little_diffusion.models import BabyUNet
from little_diffusion.solvers import LinearProbabilityPath, FlowMatchingTrainer

# ================= ğŸš€ 5070 Ti æé€Ÿæ¨¡å¼è®¾ç½® =================
# å¼€å¯ TensorFloat-32 (TF32)ï¼Œåœ¨ Ampere/Hopper æ¶æ„ä¸Šè·å¾— FP32 çš„ç²¾åº¦ + æ¥è¿‘ FP16 çš„é€Ÿåº¦
torch.set_float32_matmul_precision('high')
# å±è”½ä¸€äº›ç¼–è¯‘æ—¶çš„çƒ¦äººè­¦å‘Š
torch._dynamo.config.suppress_errors = True

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ================= ğŸ› ï¸ å‚æ•°è§£æ =================
def get_args():
    parser = argparse.ArgumentParser(description="ğŸš€ Industrial Flow Matching Trainer (Latent)")
    
    # åŸºç¡€é…ç½®
    parser.add_argument("--name", type=str, default="run", help="Experiment name")
    parser.add_argument("--data", type=str, required=True, help="Path to .pt latents file")
    parser.add_argument("--save_dir", type=str, default="./checkpoints", help="Directory to save checkpoints")
    
    # è®­ç»ƒè¶…å‚
    parser.add_argument("--epochs", type=int, default=5000)
    parser.add_argument("--batch_size", type=int, default=32, help="Try 64 or 128 for 5070 Ti")
    parser.add_argument("--lr", type=float, default=5e-4)
    parser.add_argument("--dim", type=int, default=128, help="Model width (hidden dimension)")
    
    # è¿›é˜¶åŠŸèƒ½
    parser.add_argument("--resume", type=str, default=None, help="Path to checkpoint (.pth) to resume from")
    parser.add_argument("--no_compile", action="store_true", help="Disable torch.compile (use if errors occur)")
    parser.add_argument("--save_every", type=int, default=500, help="Save checkpoint every X epochs")
    
    return parser.parse_args()

# ================= ğŸ§  æ ¸å¿ƒè®­ç»ƒé€»è¾‘ =================
def main():
    args = get_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logger.info(f"ğŸ”§ Device: {device} | Experiment: {args.name}")

    # 1. åŠ¨æ€åŠ è½½ Latent æ•°æ®
    if not os.path.exists(args.data):
        logger.error(f"âŒ Data file not found: {args.data}")
        return

    logger.info("ğŸ“¦ Loading latents into VRAM...")
    # map_location=device ç›´æ¥åŠ è½½è¿›æ˜¾å­˜ï¼Œå› ä¸º Latent æ•°æ®é€šå¸¸å¾ˆå° (<2GB)
    # å¦‚æœæ•°æ®ç‰¹åˆ«å¤§ï¼Œè¯·æ”¹ç”¨ map_location='cpu'
    latents = torch.load(args.data, map_location=device)
    
    # è‡ªåŠ¨è¯†åˆ«å°ºå¯¸ (N, 4, H, W)
    N, C, H, W = latents.shape
    logger.info(f"ğŸ“Š Dataset Shape: {latents.shape}")
    logger.info(f"   - Images: {N}")
    logger.info(f"   - Latent Size: {H}x{W} (Equivalent to Pixel {H*8}x{W*8})")

    # æ„é€  Dataset
    # å¦‚æœåªæœ‰å°‘é‡å›¾ç‰‡ï¼Œrepeat ä¸€ä¸‹è®©æ¯ä¸ª Epoch å¤šè·‘å‡ æ­¥ï¼Œé¿å… tqdm åˆ·å±å¤ªå¿«
    if N < 1000:
        repeat_factor = 1000 // N
        logger.info(f"ğŸ”„ Small dataset detected. Repeating {repeat_factor} times per epoch.")
        dataset = TensorDataset(latents.repeat(repeat_factor, 1, 1, 1))
    else:
        dataset = TensorDataset(latents)
        
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    # æ³¨æ„ï¼šin/out channels è‡ªåŠ¨è®¾ä¸º C (é€šå¸¸æ˜¯ 4)
    model = BabyUNet(in_channels=C, out_channels=C, dim=args.dim).to(device)
    
    # ğŸš€ 5070 Ti åŠ é€Ÿç¥å™¨: torch.compile
    # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šèŠ± 1-2 åˆ†é’Ÿç¼–è¯‘ï¼Œä¹‹åé€Ÿåº¦æå‡ 30%-50%
    if not args.no_compile:
        logger.info("âš¡ï¸ Compiling model with torch.compile (Mode: max-autotune)...")
        try:
            model = torch.compile(model, mode="max-autotune")
        except Exception as e:
            logger.warning(f"âš ï¸ Compile failed: {e}. Fallback to standard mode.")

    # 3. ä¼˜åŒ–å™¨ & æ··åˆç²¾åº¦ Scaler
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    scaler = torch.amp.GradScaler('cuda') # æ··åˆç²¾åº¦çš„å¤§è„‘

    start_epoch = 0

    # 4. æ–­ç‚¹ç»­è®­é€»è¾‘ (Robustness)
    if args.resume:
        if os.path.isfile(args.resume):
            logger.info(f"â™»ï¸ Resuming from checkpoint: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            
            # å¤„ç† compile å¸¦æ¥çš„å‰ç¼€é—®é¢˜
            state_dict = checkpoint['model_state_dict']
            new_state_dict = {}
            for k, v in state_dict.items():
                if k.startswith("_orig_mod."):
                    new_state_dict[k[10:]] = v
                else:
                    new_state_dict[k] = v
            
            # åŠ è½½æƒé‡
            model.load_state_dict(new_state_dict, strict=False) # strict=False å…è®¸ä¸€å®šçš„çµæ´»æ€§
            
            # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ (é‡è¦ï¼å¦åˆ™ LR ä¼šé‡ç½®)
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # æ¢å¤ Epoch
            start_epoch = checkpoint.get('epoch', 0) + 1
            logger.info(f"   -> Resuming at Epoch {start_epoch}")
        else:
            logger.warning(f"âš ï¸ Checkpoint not found: {args.resume}. Starting from scratch.")

    # 5. å‡†å¤‡è®­ç»ƒç»„ä»¶
    path = LinearProbabilityPath()
    trainer = FlowMatchingTrainer(model, path)
    
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¼˜é›…é€€å‡ºå¤„ç† (Ctrl+C)
    def signal_handler(sig, frame):
        logger.info("\nğŸ›‘ Interrupt received! Saving emergency checkpoint...")
        save_checkpoint(model, optimizer, start_epoch, save_dir / f"{args.name}_interrupted.pth")
        sys.exit(0)
    signal.signal(signal.SIGINT, signal_handler)

    # ================= ğŸ”„ è®­ç»ƒå¾ªç¯ =================
    logger.info("ğŸ”¥ Starting Training...")
    model.train()
    
    t0 = time.time()
    
    for epoch in range(start_epoch, args.epochs):
        epoch_loss = 0
        steps = 0
        
        for batch in dataloader:
            x1 = batch[0].to(device) # Target Latents
            
            optimizer.zero_grad()
            
            # âš¡ï¸ æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ (Auto Mixed Precision)
            # è¿™é‡Œçš„è®¡ç®—ä¼šè‡ªåŠ¨è½¬ä¸º FP16ï¼Œæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦ç¿»å€
            with torch.amp.autocast('cuda'):
                loss = trainer.get_train_loss(target=x1)
            
            # âš¡ï¸ Scaler åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += loss.item()
            steps += 1
            
        avg_loss = epoch_loss / steps
        
        # æ‰“å°æ—¥å¿— (æ¯ 100 è½®)
        if (epoch + 1) % 100 == 0:
            elapsed = time.time() - t0
            speed = (epoch + 1 - start_epoch) / elapsed
            logger.info(f"Epoch {epoch+1:04d} | Loss: {avg_loss:.6f} | Speed: {speed:.1f} epoch/s")

        # å®šæœŸä¿å­˜ (Robust Checkpointing)
        if (epoch + 1) % args.save_every == 0:
            save_path = save_dir / f"{args.name}_ep{epoch+1}.pth"
            save_checkpoint(model, optimizer, epoch, save_path)
            
            # åŒæ—¶ä¹Ÿæ›´æ–°ä¸€ä¸ª latest.pth æ–¹ä¾¿éšæ—¶ resume
            save_checkpoint(model, optimizer, epoch, save_dir / f"{args.name}_latest.pth")

    logger.info("âœ… Training Finished!")
    save_checkpoint(model, optimizer, args.epochs-1, save_dir / f"{args.name}_final.pth")

def save_checkpoint(model, optimizer, epoch, path):
    """ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€ï¼Œä¸ä»…ä»…æ˜¯æƒé‡"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': { # ä¿å­˜ä¸€äº›å…ƒæ•°æ®ï¼Œé˜²æ­¢ä»¥åå¿˜äº†è¿™ä¸ªæ¨¡å‹æ˜¯å•¥å‚æ•°
             'timestamp': time.time(),
        }
    }, path)
    logger.info(f"ğŸ’¾ Saved checkpoint to {path}")

if __name__ == "__main__":
    main()